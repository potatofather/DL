{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 导入必要的库\n包括数据处理、图像处理、模型构建和训练等。\n# 配置 GPU 内存增长\n提高 GPU 资源利用率，避免在训练大模型时出现内存不足的情况。","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport math\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\n    tf.config.experimental.set_memory_growth(gpu, True)\n    \nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-09T06:39:40.36543Z","iopub.execute_input":"2024-04-09T06:39:40.366052Z","iopub.status.idle":"2024-04-09T06:39:40.374317Z","shell.execute_reply.started":"2024-04-09T06:39:40.366015Z","shell.execute_reply":"2024-04-09T06:39:40.373304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 读取和预处理数据\n去除无关列，添加图像路径。确保数据集中只有相关特征，并为每个样本提供图像文件路径，以便后续加载图像数据。","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n\nsd_columns = [col for col in train.columns if col.endswith('_sd')]\ntrain = train.drop(columns=sd_columns)\n\ntrain_image_folder = '/kaggle/input/planttraits2024/train_images'\ntrain['image_path'] = train['id'].apply(lambda x: os.path.join(train_image_folder, f\"{x}.jpeg\"))\n\ntest = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\ntest_image_folder = '/kaggle/input/planttraits2024/test_images'\ntest['image_path'] = test['id'].apply(lambda x: os.path.join(test_image_folder, f\"{x}.jpeg\"))\n\nmean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n\n#limit train data for quick test\n#train = train.head(1000)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:39:42.301092Z","iopub.execute_input":"2024-04-09T06:39:42.301748Z","iopub.status.idle":"2024-04-09T06:39:44.059434Z","shell.execute_reply.started":"2024-04-09T06:39:42.301716Z","shell.execute_reply":"2024-04-09T06:39:44.058565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 定义图像增强和处理函数\n图像增强可以增加数据多样性，减少过拟合；预处理可以规范化图像数据，适应模型输入要求。\n\n","metadata":{}},{"cell_type":"code","source":"# Define image augmentation operations\ndef augment_image(img):\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_flip_up_down(img)\n    img = tf.image.random_brightness(img, max_delta=0.2)\n    img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n    img = tf.image.random_hue(img, max_delta=0.2)\n    img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n    img = tf.image.random_crop(img, size=[224, 224, 3])  # Random cropping\n    return img\n\n# Process image with augmentation\ndef process_image(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = augment_image(img)  # Apply augmentation\n    img = preprocess_input(img)\n    return img\n\n# Define your dataset processing function\ndef process_path(file_path, tabular_data, targets):\n    img = process_image(file_path)\n    return (img, tabular_data), targets","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:39:50.841491Z","iopub.execute_input":"2024-04-09T06:39:50.842286Z","iopub.status.idle":"2024-04-09T06:39:50.850236Z","shell.execute_reply.started":"2024-04-09T06:39:50.842254Z","shell.execute_reply":"2024-04-09T06:39:50.849128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据可视化\n了解数据分布情况","metadata":{}},{"cell_type":"code","source":"def plot_data(df):\n    plt.figure(figsize=(15, 3))\n\n    # Setting up a grid of plots with 2 columns\n    n_cols = 6\n    n_rows = len(mean_columns) // n_cols + (len(mean_columns) % n_cols > 0)\n\n    for i, col in enumerate(mean_columns):\n        plt.subplot(n_rows, n_cols, i+1)\n        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n        plt.title(f'Distribution of {col}')\n        plt.xlabel('Value')\n        plt.ylabel('Density')\n\n    plt.tight_layout()\n    plt.show()\n    \nplot_data(train)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:40:07.652036Z","iopub.execute_input":"2024-04-09T06:40:07.652398Z","iopub.status.idle":"2024-04-09T06:40:11.105371Z","shell.execute_reply.started":"2024-04-09T06:40:07.652369Z","shell.execute_reply":"2024-04-09T06:40:11.104395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据清洗\n去除异常值，提升模型的训练效果。","metadata":{}},{"cell_type":"code","source":"for column in mean_columns:\n    upper_quantile = train[column].quantile(0.98)  \n    train = train[(train[column] < upper_quantile)]\n    train = train[(train[column] > 0)]    \nplot_data(train)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:40:11.107166Z","iopub.execute_input":"2024-04-09T06:40:11.107697Z","iopub.status.idle":"2024-04-09T06:40:14.114194Z","shell.execute_reply.started":"2024-04-09T06:40:11.107661Z","shell.execute_reply":"2024-04-09T06:40:14.113237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据标准化\n使每个特征的均值为0，标准差为1。","metadata":{}},{"cell_type":"code","source":"original_means = {}\noriginal_stds = {}\n\nfor column in mean_columns:\n    # Calculate the mean and standard deviation for each column\n    original_means[column] = train[column].mean()\n    original_stds[column] = train[column].std()\n    \n    # Apply the scaling: (value - mean) / std\n    # This standardizes each column to have a mean of 0 and std of 1\n    train[column] = (train[column] - original_means[column]) / original_stds[column]\n    \nplot_data(train)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:35:24.286323Z","iopub.execute_input":"2024-04-09T06:35:24.286951Z","iopub.status.idle":"2024-04-09T06:35:26.982557Z","shell.execute_reply.started":"2024-04-09T06:35:24.286915Z","shell.execute_reply":"2024-04-09T06:35:26.981679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 归一化辅助数据\n将数据缩放到相同范围，提高模型的训练效果。","metadata":{}},{"cell_type":"code","source":"x = train.drop(columns=['id', 'image_path'] + mean_columns)\n\nfor column in x.columns:\n    min_val = x[column].min()\n    max_val = x[column].max()\n    x[column] = (x[column] - min_val) / (max_val - min_val)    ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:35:29.688694Z","iopub.execute_input":"2024-04-09T06:35:29.689355Z","iopub.status.idle":"2024-04-09T06:35:29.841653Z","shell.execute_reply.started":"2024-04-09T06:35:29.689323Z","shell.execute_reply":"2024-04-09T06:35:29.840672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 拆分数据集，创建 TensorFlow 数据集","metadata":{}},{"cell_type":"code","source":"y = train[mean_columns]\nx_paths = train['image_path']\n\ntrain_tabular, val_tabular, train_targets, val_targets = train_test_split(\n    x, y, test_size=0.2, random_state=42)\n\ntrain_paths, val_paths = train_test_split(\n    x_paths, test_size=0.2, random_state=42)\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_tabular.to_numpy(), train_targets.to_numpy()))\nval_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_tabular.to_numpy(), val_targets.to_numpy()))\n\n# Apply the processing function\ntrain_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Batch and prefetch\ntrain_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:36:15.134532Z","iopub.execute_input":"2024-04-09T06:36:15.134909Z","iopub.status.idle":"2024-04-09T06:36:15.908204Z","shell.execute_reply.started":"2024-04-09T06:36:15.13488Z","shell.execute_reply":"2024-04-09T06:36:15.906823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 构建和编译模型: \n构建并编译一个多输入多输出模型，包括图像特征提取路径和辅助数据处理路径。","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the image model pathway\nimage_input = Input(shape=(224, 224, 3))\n\n# Custom CNN architecture\nimage_feature_layers = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\nimage_feature_layers = MaxPooling2D((2, 2))(image_feature_layers)\nimage_feature_layers = Conv2D(64, (3, 3), activation='relu', padding='same')(image_feature_layers)\nimage_feature_layers = MaxPooling2D((2, 2))(image_feature_layers)\nimage_feature_layers = Conv2D(128, (3, 3), activation='relu', padding='same')(image_feature_layers)\nimage_feature_layers = MaxPooling2D((2, 2))(image_feature_layers)\nimage_feature_layers = Flatten()(image_feature_layers)\nimage_feature_layers = Dense(256, activation='relu')(image_feature_layers)\nimage_feature_layers = Dropout(0.3)(image_feature_layers)  # Add dropout for regularization\n\ntabular_input_shape = x.shape[1]\ntarget_columns_shape = y.shape[1]\n\n# Define the tabular model pathway\ntabular_input = Input(shape=(tabular_input_shape,))\ntabular_dense = Dense(512, activation='relu')(tabular_input)\ntabular_dense = Dropout(0.3)(tabular_dense)  # Add dropout for regularization\n\n# Concatenate both pathways\nconcat = Concatenate()([image_feature_layers, tabular_dense])\nconcat_dense = Dense(256, activation='relu')(concat)\nconcat_dense = Dropout(0.3)(concat_dense)  # Continue to use dropout for regularization\n\n# Output layer for 6 targets (assuming 'mean_columns' is your output size)\noutput = Dense(target_columns_shape, activation='linear')(concat_dense)  # Use linear activation for regression\n\n# Create the model\nmodel = Model(inputs=[image_input, tabular_input], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n\n# Display the model summary to check the architecture\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:35:34.569859Z","iopub.execute_input":"2024-04-09T06:35:34.570546Z","iopub.status.idle":"2024-04-09T06:35:34.675975Z","shell.execute_reply.started":"2024-04-09T06:35:34.570514Z","shell.execute_reply":"2024-04-09T06:35:34.675142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练模型","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_ds, validation_data=val_ds, epochs=30)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:08.311462Z","iopub.execute_input":"2024-04-09T06:34:08.311702Z","iopub.status.idle":"2024-04-09T06:34:56.371025Z","shell.execute_reply.started":"2024-04-09T06:34:08.311679Z","shell.execute_reply":"2024-04-09T06:34:56.35886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 保存模型","metadata":{}},{"cell_type":"code","source":"model.save(\"model.keras\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:56.372284Z","iopub.status.idle":"2024-04-09T06:34:56.373358Z","shell.execute_reply.started":"2024-04-09T06:34:56.373033Z","shell.execute_reply":"2024-04-09T06:34:56.373069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 预处理测试数据\n确保测试数据的处理与训练数据一致，方便模型进行预测。","metadata":{}},{"cell_type":"code","source":"# Prepare tabular data (excluding 'id' and 'image_path')\ntest_tabular = test.drop(columns=['id', 'image_path'])\n\n#normalize tabular data\nfor column in test_tabular.columns:\n    min_val = test_tabular[column].min()\n    max_val = test_tabular[column].max()\n    test_tabular[column] = (test_tabular[column] - min_val) / (max_val - min_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:56.374276Z","iopub.status.idle":"2024-04-09T06:34:56.374676Z","shell.execute_reply.started":"2024-04-09T06:34:56.37447Z","shell.execute_reply":"2024-04-09T06:34:56.374485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 进行预测","metadata":{}},{"cell_type":"code","source":"test_tabular_np = test_tabular.to_numpy()\n\n# Create a TensorFlow dataset for the image paths and map them through the preprocessing function\ntest_images_ds = tf.data.Dataset.from_tensor_slices(test['image_path'])\\\n    .map(process_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# Create a TensorFlow dataset for the tabular data\ntest_tabular_ds = tf.data.Dataset.from_tensor_slices(test_tabular_np)\n\n# Zip the two datasets together\ntest_ds = tf.data.Dataset.zip((test_images_ds, test_tabular_ds))\n\n# Prepare the dataset for prediction by ensuring the structure matches the model's expectations\ntest_ds_for_prediction = test_ds.map(lambda image, tabular: ((image, tabular),), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# Batch the dataset\ntest_ds_batched = test_ds_for_prediction.batch(32)\n\n# Use the model to predict on the batched dataset\npredictions = model.predict(test_ds_batched)\n\npredictions_df = pd.DataFrame(predictions, columns=mean_columns)\ntest = pd.concat([test.reset_index(drop=True), predictions_df], axis=1)\n\nplot_data(test)\n\n#Verify we didn't predict and NaNs..\nprint(\"NaN values\\n\", test[mean_columns].isna().sum())\ntest[mean_columns]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:56.377066Z","iopub.status.idle":"2024-04-09T06:34:56.377459Z","shell.execute_reply.started":"2024-04-09T06:34:56.377262Z","shell.execute_reply":"2024-04-09T06:34:56.377279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 反归一化预测值/调整到原始缩放比例\n将预测结果转化回原始尺度，方便理解和进一步使用。","metadata":{}},{"cell_type":"code","source":"for column in mean_columns:\n    original_mean = original_means[column]\n    original_std = original_stds[column]\n\n    # Reverse the standardization\n    test[column] = test[column] * original_std + original_mean\n\nplot_data(test)\ntest[mean_columns]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:56.379163Z","iopub.status.idle":"2024-04-09T06:34:56.379501Z","shell.execute_reply.started":"2024-04-09T06:34:56.379338Z","shell.execute_reply":"2024-04-09T06:34:56.379352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 保存结果，保存为.csv文件","metadata":{}},{"cell_type":"code","source":"test = test[['id'] + mean_columns]\n\n#rename from _mean\ntest.columns = test.columns.str.replace('_mean', '')\ntest.to_csv('submission.csv', index=False)\n\ntest","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:34:56.381456Z","iopub.status.idle":"2024-04-09T06:34:56.381918Z","shell.execute_reply.started":"2024-04-09T06:34:56.381659Z","shell.execute_reply":"2024-04-09T06:34:56.381676Z"},"trusted":true},"execution_count":null,"outputs":[]}]}